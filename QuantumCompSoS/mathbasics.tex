\section{Mathematical Preliminaries}

We assume that the reader is familiar with basic linear algebra that is covered during the first year. As our treatment of Quantum Computation heavily uses this, we will summarise some key theorems and notations that we will be using.

\subsection{Vectors and Vector Spaces}
The most important structure in linear algebra is a vector which we will represent as $\ket{\psi}$. This vector resides in a vector space that satisfies some axioms that give this structure it's core defining properties. This is famously known as the \textit{bra-ket} notation and the $\ket{.}$ is used to represent a vector. The same vector in the vector space $\mathbf{C}^{n}$ is conveniently represented as a column vector.

We will be generally working in the vector space $\mathbf{C}^{n}$ over the field $\mathbf{C}$.
A set of vectors $\ket{\psi_{1}}, \ket{\psi_{2}} \cdots \ket{\psi_{n}}$ is said to be \textit{linearly-independent} if for any set of coefficients ${a_i}$, 
$$\sum_{i=1}^{n} a_i\ket{\psi_{i}} = 0 \implies a_i = 0   \forall   1 \leq i \leq n$$
A set of vectors ${\ket{\psi_{i}}}$ is said to be a spanning set of a vector space $\textbf{W}$ if any vector in $\textbf{W}$ can be represented as a linear combination of that set. Note that such a set may be non-finite however we will be dealing with finite dimensional vector spaces only. If this set is linearly independent then this set is called a \textit{basis} of that vector space. It is left as an exercise to show that the cardinality of all bases are same and this value is termed as the \textit{dimension} of that vector space.

\subsection{Linear Operators and Matrices}
A linear operator $A$ from vector space \textbf{V} to \textbf{W} is a function mapping vectors from \textbf{V} to \textbf{W} satisfying linearity, that is:
$$ A(\sum_{i} a_i \ket{\psi_{i}}) = \sum_{i} a_{i}A\ket{\psi_{i}} $$ 
We can then define compositions of operators, $BA$ as function that first operates $A$ and then operates $B$. 

It is easy to see that matrix multiplication is a linear operator. Conversely any linear operator has a matrix representation. To see this, let $A: V -> W$ be a linear operator. Let $\ket{v_{i}}$ be an ordered basis (basis formed by fixing order of it's elements, in this case by enforcing an order restriction on $\ket{v_{i}}$.) of $V$ and $\ket{w_{j}}$ be an ordered basis of $W$.

Then there exists complex numbers $A_{i,j}$ such that: $$A\ket{v_{i}} = \sum_{j} A_{i,j}\ket{w_{j}}$$.
If we represent any vector in $V$ by it's coordinate vector representation, that is form a column vector whose elements with respect to an ordered basis are the scalar coefficients along the basis elements  and multiply it by the matrix formed by $A_{i,j}$ we get the coordinate vector representation of the output vector in the ordered basis $\ket{w_{j}}$.

\subsection{Inner Products}
An inner product of a vector space \textbf{V} is defined as a function from $\textbf{V} \cross \textbf{V}$ to $\textbf{C}$. We will denote the inner product of $\ket{v}$ and $\ket{w}$ as $(\ket{v}, \ket{w})$ or as $\braket{v}{w}$. The inner product satisfies the following properties:
\begin{enumerate}
    \item $\braket{v}{w} = ({\braket{w}{v}})^{*}$
    \item $\braket{v}{\sum_{j} a_{j}w_{j}} = \sum_{j} a_{j} \braket{v}{w_{j}}$
    \item $\braket{v}{v} \geq 0 $ with equality \textit{iff} $v=0$
\end{enumerate}
We can now define the \textit{length} or \textit{norm} of a vector 
$\norm{\ket{v}} = \sqrt{\braket{v}{v}}$. 
Two vectors are said to be orthogonal if their inner product is $0$. 
We now define an \textit{orthonormal} basis as a basis $\ket{v_{i}}$ whose elements have unit norm and they are pairwise orthogonal, that is for $i\neq j \braket{v_i}{v_j} = 0$.
It can be proven that every finite dimensional vector space has an orthonormal basis. This is left as an exercise. The procedure for determining this orthonormal set from any basis is termed as the \textit{Gram-Schmidt} orthogonalisation process.


